{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratorio 1 - Parte 2\n",
    "\n",
    "### Regresión logística y Funciones Discriminantes Gausianas\n",
    "\n",
    "### 2018-II\n",
    "\n",
    "#### Profesor: Julián D. Arias Londoño\n",
    "#### julian.ariasl@udea.edu.co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guía del laboratorio\n",
    "\n",
    "En este archivo va a encontrar tanto celdas de código cómo celdas de texto con las instrucciones para desarrollar el laboratorio.\n",
    "\n",
    "Lea atentamente las instrucciones entregadas en las celdas de texto correspondientes y proceda con la solución de las preguntas planteadas.\n",
    "\n",
    "Nota: no olvide ir ejecutando las celdas de código de arriba hacia abajo para que no tenga errores de importación de librerías o por falta de definición de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haga click en el siguiente enlace para autenticarse con su cuenta de correo institucional\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=893762525034-g9d91ddls9e19a1q77c7hsq2rhgqo9h7.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080&scope=profile+email&access_type=offline&response_type=code\n",
      "waiting for authentication ...\n",
      "authentication succeeded\n",
      "/?code=4/OgAmZEtvp3NxQg4QXbcDPVDo4fxVXyH3bNBDOzllUbsOn9vtfDWRFGqjioW6gALPCnjqhN_158dC4eJKjm6ZiyQ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='https://lh5.googleusercontent.com/-iIKWTH2jxlM/AAAAAAAAAAI/AAAAAAAAANs/Rib3FMleaPE/photo.jpg' width=60 height=60/></td><td>davida.marin@udea.edu.co<br/>DAVID   ALEJANDRO MARÍN ALZATE<br/>google id: 100532802635099902978<br/></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tomado de https://github.com/rramosp/mooc-grader\n",
    "from Autentication import *#python 3\n",
    "import inspect, urllib\n",
    "html, auth_code, userinfo = google_authenticate(PORT_NUMBER=8080)\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda integrante\n",
    " Deiry Sofía Navas Muriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from __future__ import division\n",
    "\n",
    "#Algunas advertencias que queremos evitar\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "En este laboratorio se va a realizar un procedimiento análogo al del laboratorio anterior, pero con el modelo de regresión logística que sirve para resolver problemas de clasificación (en principio biclase).\n",
    "\n",
    "Analice los siguientes métodos a la luz de la teoría vista para el modelo de regresión logística. Una vez comprenda su funcionamiento proceda a completar el código del método de gradiente descendente con la regla de actualización de los parámetros\n",
    "\n",
    "$$w_j(iter) = w_j(iter-1) - \\eta \\frac{\\partial E(w)}{\\partial w_j}$$ \n",
    "\n",
    "Para el problema de clasificación. Tenga presente que si ya implementó la regla de actualización de parámetros para el modelo de regresión polinomial múltiple, este punto es trivial, ya que solo tiene que incluir la función sigmoidal tal como lo vimos en la teoría.\n",
    "\n",
    "Además se pide graficar el error de clasificación durante las iteraciones del algorítmo. La gráfica debe llevar título y los correspondientes nombres de los ejes.\n",
    "\n",
    "Nota: observe que el método logistic_regression ya hace el llamado a la función sigmoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoidal\n",
    "def sigmoidal(z):\n",
    "    \n",
    "    #Complete la siguiente línea con el código para calcular la salida de la función sigmoidal\n",
    "    # s = 1/(1+np.exp(-z))\n",
    "    s = np.exp(z)/(1+np.exp(z))\n",
    "    \n",
    "    #Complete el código para realizar la gráfica de la función aquí\n",
    "    #u = np.linspace(-10,10,100)\n",
    "    #plt.plot(u,s)\n",
    "    #plt.show()\n",
    "    return s\n",
    "\n",
    "#Sigmoidal\n",
    "def sigmoidalVec(z):\n",
    "    \n",
    "    for iter in range(0,len(z)):\n",
    "        #Complete la siguiente línea con el código para calcular la salida de la función sigmoidal\n",
    "        # s = 1/(1+np.exp(-z))\n",
    "        z[iter] = np.exp(z[iter]) / (1+np.exp(z[iter]))\n",
    "\n",
    "        #Complete el código para realizar la gráfica de la función aquí\n",
    "        #u = np.linspace(-10,10,100)\n",
    "        #plt.plot(u,s)\n",
    "        #plt.show()\n",
    "    return z\n",
    "\n",
    "\n",
    "#Modelo Regresión logística\n",
    "def logistic_regression(X, W):\n",
    "    Yest = np.dot(X,W)  #con np.dot se realiza el producto matricial. Aquí X (extendida) tiene dim [Nxd] y W es dim [dx1]\n",
    "    Y_lest = sigmoidal(Yest)\n",
    "    \n",
    "    #Se llevan los valores a 1 o 0 para los que está definido el modelo de regresión logística\n",
    "    pos = 0\n",
    "    for tag in Y_lest:\n",
    "        \n",
    "        if tag > 0.5:\n",
    "            Y_lest[pos] = 1\n",
    "        elif tag < 0.5:\n",
    "            Y_lest[pos] = 0\n",
    "        \n",
    "        pos += 1\n",
    "    \n",
    "    return Y_lest    #Esta variable contiene la salida de sigm(f(X,W))\n",
    "\n",
    "\n",
    "#Potencia de polinomio (En es laboratorio solo trabajaremos el caso lineal (grado 1), pero se pueden probar otras fronteras)\n",
    "def potenciaPolinomio(X,grado):\n",
    "    X2 = X\n",
    "    \n",
    "    if grado != 1:\n",
    "        for i in range(2,grado+1):\n",
    "            Xadd = X**i\n",
    "            X2 = np.concatenate((X2, Xadd), axis=1)\n",
    "    \n",
    "    return X2\n",
    "\n",
    "\n",
    "#Para calcular el error del modelo de regresión logística\n",
    "def error_logistic(Y_lest, Y):\n",
    "    error = 0\n",
    "    for ye, y in zip(Y_lest, Y):\n",
    "        if ye != y:\n",
    "            error += 1\n",
    "    \n",
    "    error = error/np.size(Y)\n",
    "    \n",
    "    #print (\"La eficiencia en esta iteración fue: \"+str(1-error)+'\\n')\n",
    "    \n",
    "    return error\n",
    "\n",
    "#Gradiente descendente para regresión logística\n",
    "def gradiente_descendente_logistic(X,Y,grado,eta,isShowGrap=True):\n",
    "    \n",
    "    #X es la matriz de datos extendida. W es el vector de parámetros del modelo\n",
    "    #Extendemos la matriz\n",
    "    unos = np.array([np.ones(np.size(X,0))])\n",
    "    #Concatenar el vector de unos con la matriz X\n",
    "    X = np.concatenate((unos.T, X), axis=1)\n",
    "    X = X.reshape(np.size(X,0),np.size(X,1))\n",
    "    \n",
    "    Y = Y.reshape(np.size(Y), 1)\n",
    "    \n",
    "    #Tomamos el número de variables del problema\n",
    "    d = np.size(X,1)\n",
    "\n",
    "    #Tomamos el número de muestras de la base de datos\n",
    "    N = np.size(X,0)\n",
    "    \n",
    "    #Inicializamos el vector de parámetros aleatoriamente\n",
    "    #Want = np.random.randn(d)\n",
    "    W = np.zeros(d)\n",
    "    W = W.reshape(np.size(W),1)\n",
    "\n",
    "    eta = eta\n",
    "    \n",
    "    iteraciones = 1000\n",
    "    errores = np.zeros(iteraciones)\n",
    "    \n",
    "    for iter in range(iteraciones):\n",
    "\n",
    "        Y_estimado = logistic_regression(X,W)\n",
    "        #Error en clasificación\n",
    "        error = error_logistic(Y_estimado,Y)\n",
    "        errores[iter] = error\n",
    "        \n",
    "        #Aquí debe completar el código con la regla de actualización de los parámetros W para regresión\n",
    "        #logística. Tenga en cuenta los nombres de las variables ya creadas: Want, X, Y\n",
    "        \n",
    "        for j in range(0, d):\n",
    "            s = 0\n",
    "            #for i in range(0, N):\n",
    "             #   s +=  sigmoidal( np.dot(X[i,:],W) - Y[i] ) * X[i,j]\n",
    "            s = np.dot(( sigmoidalVec(np.dot(X, W) ) - Y).T , X[:,j])\n",
    "                       \n",
    "            W[j] = W[j] - eta * (s / N)\n",
    "            \n",
    "        #se imprime cada 100 iteracciones el error    \n",
    "        if iter%100 == 0 or iter == 0:\n",
    "            print(\"Iteración: \",iter,\" ECM: \",errores[iter])     \n",
    "            \n",
    "    \n",
    "    #Aquí debe completar el código para realizar la gráfica del error de clasificación vs. iteraciones\n",
    "    if(isShowGrap):\n",
    "        plt.plot(errores, color='green')\n",
    "        plt.xlabel(\"Iteraciones\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.xlim(0,iteraciones)\n",
    "        plt.show()\n",
    "   \n",
    "    \n",
    "    \n",
    "    # print ('Vector de parámetros del modelo:\\n')\n",
    "    # print (W)\n",
    "    print ('\\nError de entrenamiento = ' + str(errores[-1]))\n",
    " \n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "A continuación se leen los datos de un problema de clasificación. Las variables o caracterísicas son guardadas en la variable X y la variable de salida es guardada en la variable Y. Grafique los datos usando la funci&oacute;n scatter de matplotlib y responda a las siguientes preguntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('DB/DatosClases.mat')\n",
    "X = mat['X'] # Muestras x características\n",
    "Y = mat['Y'] #Variable de salida\n",
    "\n",
    "X2 = X[:100][:,:2]\n",
    "y2 = Y[:100]\n",
    "#plt.scatter(X2[:,0], X2[:,1], c=y2,cmap=\"Accent\");\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#plt.scatter(X, Y)\n",
    "#plt.xlabel(\"Muestras\")\n",
    "#plt.ylabel(\"Variable\")\n",
    "#plt.legend(loc=2)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Cu&aacute;ntas clases tiene el problema?: La variable salida presenta dos clases uno y cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Cu&aacute;ntas caracter&iacute;sticas tiene el problema?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(X,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Cu&aacute;ntas muestras tiene el problema?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(X,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 El problema es linealmente separable?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.92606402 -6.83699086]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(X[0,:])\n",
    "print(Y[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Complete el código de la siguiente celda llamando el método gradiente_descendente_logistic y pasándole los parámetros correspondientes, de acuerdo con los parámetros que indica la tabla de resultados, ejecute el entrenamiento y llene la tabla de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.002857142857142857\n",
      "Iteración:  200  ECM:  0.0\n",
      "Iteración:  300  ECM:  0.0\n",
      "Iteración:  400  ECM:  0.0\n",
      "Iteración:  500  ECM:  0.0\n",
      "Iteración:  600  ECM:  0.0\n",
      "Iteración:  700  ECM:  0.0\n",
      "Iteración:  800  ECM:  0.0\n",
      "Iteración:  900  ECM:  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFGRJREFUeJzt3X/YX3V93/HnKwk/VBBFsCU/MLGN2lzMFpYhVEdpRRpYG2ZHK1m9qsLVrJ1QN1u94Oqu2bJdq9auna7MmXVKi5s0/pjLrGtckVYvKzaJKCNBNAVskspCEWmV1hDy3h/fc5MvN0nu+8D35OTc9/NxXd/r/p7P+eR839+Tk7zucz7nR6oKSZJma0HfBUiShsXgkCS1YnBIkloxOCRJrRgckqRWDA5JUisGhySpFYNDktSKwSFJamVR3wW0ddppp9Xy5cv7LkOSBmXbtm1/VVWnT2JZgwuO5cuXs3Xr1r7LkKRBSfK1SS3LQ1WSpFYMDklSKwaHJKkVg0OS1IrBIUlqpbPgSPK+JHuT3HmY+Uny7iQ7k9yR5JyuapEkTU6Xexw3AmuOMP8SYGXzWg+8p8NaJEkT0llwVNWngW8coctlwO/VyG3Ac5KcMdNy935776RKlCQ9BX1eALgE2DU2vbtp+/r0jknWM9or4bjFxx2V4iRJhzaIwfGq2lBVq6tq9cJFC/suR5LmtT6DYw+wbGx6adMmSTqG9Rkcm4Cfac6uOg94uKqedJhKknRs6WyMI8kHgQuB05LsBt4GHAdQVf8Z+ARwKbATeAR4Q1e1SJImp7PgqKp1M8wv4I1dfb4kqRuDGBx/guq7AEma34YXHJKkXhkckqRWDA5JUiuDC45ykEOSejW44JAk9cvgkCS1YnBIkloxOCRJrRgckqRWDA5JUisGhySpFYNDktSKwSFJasXgkCS1YnBIkloZXHB4rypJ6tfggkOS1C+DQ5LUisEhSWrF4JAktWJwSJJaGV5weFKVJPVqeMEhSeqVwSFJasXgkCS1YnBIkloZXHB4yxFJ6tfggkOS1C+DQ5LUSqfBkWRNkruT7Exy7SHmn5nk1iS3J7kjyaVd1iNJevo6C44kC4EbgEuAVcC6JKumdftXwMaqOhu4AvhPXdUjSZqMLvc4zgV2VtU9VbUPuBm4bFqfAp7dvD8F+MsO65EkTcCiDpe9BNg1Nr0beNm0Pr8CfDLJNcCzgIs6rEeSNAF9D46vA26sqqXApcBNSZ5UU5L1SbYm2XrgwIGjXqQk6aAug2MPsGxsemnTNu4qYCNAVX0OOBE4bfqCqmpDVa2uqtULFvSddZI0v3X5v/AWYGWSFUmOZzT4vWlan78AXgmQ5PsYBccDHdYkSXqaOguOqtoPXA1sBu5idPbU9iTXJ1nbdPtF4GeTfAn4IPD6qvLScEk6hmVo/08ft+y4enTXo32XIUmDkmRbVa2exLIcMJAktWJwSJJaMTgkSa0YHJKkVgwOSVIrBockqZXhBcewzh6WpDlneMEhSeqVwSFJasXgkCS1YnBIkloZXHCUo+OS1KvBBYckqV8GhySpFYNDktSKwSFJasXgkCS1YnBIkloxOCRJrRgckqRWDA5JUisGhySplcEFh7cckaR+DS44JEn9MjgkSa0YHJKkVgwOSVIrBockqRWDQ5LUyvCCw7NxJalXwwsOSVKvOg2OJGuS3J1kZ5JrD9Pnp5LsSLI9yX/vsh5J0tO3qKsFJ1kI3AC8CtgNbEmyqap2jPVZCVwHvLyqHkry/K7qkSRNRpd7HOcCO6vqnqraB9wMXDatz88CN1TVQwBVtXemhXrLEUnqV5fBsQTYNTa9u2kb9yLgRUk+m+S2JGsOtaAk65NsTbLV3JCkfnV2qKrF568ELgSWAp9O8veq6pvjnapqA7ABYMGSBUaHJPWoyz2OPcCysemlTdu43cCmqnq0qu4FvsIoSCRJx6gug2MLsDLJiiTHA1cAm6b1+RijvQ2SnMbo0NU9HdYkSXqaOguOqtoPXA1sBu4CNlbV9iTXJ1nbdNsMPJhkB3Ar8JaqerCrmiRJT1+qhjVksGDJgjqw50DfZUjSoCTZVlWrJ7EsrxyXJLVicEiSWjE4JEmtGBySpFZmDI4kC5P8xtEoRpJ07JsxOKrqMeAVR6GWWfFeVZLUr9necuT2JJuADwHfnmqsqo92UpUk6Zg12+A4EXgQ+JGxtgIMDkmaZ2YVHFX1hq4LkSQNw6zOqkqyNMn/SLK3eX0kydKui5MkHXtmezru+xndoHBx8/pfTZskaZ6ZbXCcXlXvr6r9zetG4PQO6zo8T6qSpF7NNjgeTPLa5pqOhUley2iwXJI0z8w2OK4Efgq4H/g6cDnggLkkzUMznlWVZCHwE1W1dqa+kqS5b7ZXjq87CrVIkgZgthcAfjbJbwO/zxOvHP9CJ1VJko5Zsw2OH2h+Xj/WVjzxSnJJ0jwwmzGOBcB7qmrjUahnVqqKJH2XIUnz0mzGOA4Abz0KtUiSBmC2p+P+UZJfSrIsyalTr04rOwJvrS5J/ZntGMdrmp9vHGsr4IWTLUeSdKyb7d1xV3RdiCRpGI54qCrJW8fe/+S0ef+uq6JmUuWhKknqy0xjHFeMvb9u2rw1E65FkjQAMwVHDvP+UNNHjYPjktSfmYKjDvP+UNOSpHlgpsHx70/y14z2Lp7RvKeZPrHTyo7AMQ5J6s8Rg6OqFh6tQiRJwzDbCwAlSQI6Do4ka5LcnWRnkmuP0O+fJKkkq2ezXAfHJak/nQVH8wCoG4BLgFXAuiSrDtHvZOBNwOe7qkWSNDld7nGcC+ysqnuqah9wM3DZIfr9G+AdwN/NdsEOjktSf7oMjiXArrHp3U3b45KcAyyrqj/osA5J0gTN9iaHE9c85+M3gdfPou96YD0AZzjGIUl96nKPYw+wbGx6adM25WTgLOCPk9wHnAdsOtQAeVVtqKrVVTWrwXNJUne6DI4twMokK5Icz+i+V5umZlbVw1V1WlUtr6rlwG3A2qraOtOCHeOQpP50FhxVtR+4GtgM3AVsrKrtSa5Psrarz5UkdStD++09i1OPfO0RnnHcM/ouRZIGI8m2SR3uH+SV4w6OS1J/BhkckqT+DDI4hnZ4TZLmkkEGhySpP4MMDsc4JKk/gwwOSVJ/BhkcjnFIUn8GGRySpP4YHJKkVgYZHA6OS1J/BhkckqT+DDI4HByXpP4MMjgkSf0ZZHA4xiFJ/RlkcEiS+jPI4HCMQ5L6M8jgkCT1x+CQJLUyyOBwcFyS+jPI4JAk9WeQweHguCT1Z5DBIUnqzyCDwzEOSerPIINDktQfg0OS1Mogg8PBcUnqzyCDQ5LUn0EGh4PjktSfQQaHJKk/gwwOxzgkqT+DDA5JUn86DY4ka5LcnWRnkmsPMf/NSXYkuSPJLUleMJvlOsYhSf3pLDiSLARuAC4BVgHrkqya1u12YHVVvRT4MPDrXdUjSZqMLvc4zgV2VtU9VbUPuBm4bLxDVd1aVY80k7cBSzusR5I0AV0GxxJg19j07qbtcK4C/vehZiRZn2Rrkq3g4Lgk9WlR3wUAJHktsBr4oUPNr6oNwAaALI6pIUk96jI49gDLxqaXNm1PkOQi4JeBH6qq78xmwQ6OS1J/ujxUtQVYmWRFkuOBK4BN4x2SnA28F1hbVXs7rEWSNCGdBUdV7QeuBjYDdwEbq2p7kuuTrG26vRM4CfhQki8m2XSYxU1fdic1S5JmlqH9J5zFqd1f3s2SZx9pnF2SNC7JtqpaPYllDfLKccc4JKk/gwwOSVJ/DA5JUiuDDI6hjctI0lwyyOCQJPVnkMHh4Lgk9WeQwSFJ6s8gg8MxDknqzyCDQ5LUn0EGh2McktSfQQaHJKk/BockqZVBBoeD45LUn0EGhySpP4MMDgfHJak/gwwOSVJ/BhkcjnFIUn8GGRySpP4YHJKkVgYZHA6OS1J/BhkckqT+DDI4HByXpP4MMjgkSf0ZZHA4xiFJ/RlkcEiS+jPI4HCMQ5L6M8jgkCT1x+CQJLUyyOBwcFyS+jPI4JAk9WdRlwtPsgZ4F7AQ+J2qevu0+ScAvwf8feBB4DVVdd9My33z5jfz7BOe/fj0y5e9nGteds0EK5ckHU66OkMpyULgK8CrgN3AFmBdVe0Y6/PPgZdW1c8luQJ4dVW95kjLfeYLnllnvuXMx6fv++Z9fOex73DBCy7gB5f+4JP6L8gCrjz7Sr7n1O+ZyPeSpCFKsq2qVk9kWR0Gx/nAr1TVjzbT1wFU1a+N9dnc9PlckkXA/cDpdYSiVq9eXVu3bn18et9j+7hq01Vs3L7xkP33PbYPgBMXnTi7ugk//uIf55UrXjmr/kfLDy//YVY+b2XfZUgaqEkGR5eHqpYAu8amdwMvO1yfqtqf5GHgecBfzfZDjl94PDe9+iZuevVNh5x/5947+cAdH5j1tR9/8rU/YeP2jYcNoj6tOn1V3yVIUrdjHJOSZD2wHuDMM8+cofcTnfX8s3j7RW+fueOY+791PwfqQKs/06V7HrqHDds28Lf7/7bvUiQN1A52zNxplroMjj3AsrHppU3bofrsbg5VncJokPwJqmoDsAFGh6o6qXbMd5/03V1/RCuLT17MK858Rd9lSBqwkIktq8vTcbcAK5OsSHI8cAWwaVqfTcDrmveXA5860viGJKl/ne1xNGMWVwObGZ2O+76q2p7kemBrVW0C/itwU5KdwDcYhYsk6RjW6RhHVX0C+MS0tn899v7vgJ/ssgZJ0mR55bgkqRWDQ5LUisEhSWrF4JAktWJwSJJa6exeVV1J8jfA3X3XcYw4jRa3Z5njXBcHuS4Ocl0c9OKqOnkSCxrELUemuXtSN+oauiRbXRcjrouDXBcHuS4OSrJ15l6z46EqSVIrBockqZUhBseGvgs4hrguDnJdHOS6OMh1cdDE1sXgBsclSf0a4h6HJKlHgwqOJGuS3J1kZ5Jr+66nS0mWJbk1yY4k25O8qWk/Ncn/SfLV5udzm/YkeXezbu5Ick6/32DykixMcnuSjzfTK5J8vvnOv9/cvp8kJzTTO5v5y/use9KSPCfJh5N8OcldSc6fr9tFkn/Z/Pu4M8kHk5w4X7aLJO9LsjfJnWNtrbeDJK9r+n81yesO9VnTDSY4kiwEbgAuAVYB65LM5Wep7gd+sapWAecBb2y+77XALVW1ErilmYbRelnZvNYD7zn6JXfuTcBdY9PvAH6rqr4XeAi4qmm/Cnioaf+tpt9c8i7gD6vqJcD3M1on8267SLIE+AVgdVWdxejxDVcwf7aLG4E109pabQdJTgXexuix3ucCb5sKmyOqqkG8gPOBzWPT1wHX9V3XUfz+/xN4FaOLH89o2s5gdF0LwHuBdWP9H+83F16MniB5C/AjwMeBMLqwa9H07YPRM2DOb94vavql7+8wofVwCnDv9O8zH7cLYAmwCzi1+Xv+OPCj82m7AJYDdz7V7QBYB7x3rP0J/Q73GsweBwc3kim7m7Y5r9mlPhv4PPBdVfX1Ztb9wHc17+f6+vkPwFuBqYfBPw/4ZlXtb6bHv+/j66KZ/3DTfy5YATwAvL85bPc7SZ7FPNwuqmoP8BvAXwBfZ/T3vI35uV1MabsdPKXtY0jBMS8lOQn4CPAvquqvx+fV6FeEOX9aXJIfA/ZW1ba+azkGLALOAd5TVWcD3+bg4QhgXm0XzwUuYxSmi4Fn8eRDN/NWl9vBkIJjD7BsbHpp0zZnJTmOUWj8t6r6aNP8/5Kc0cw/A9jbtM/l9fNyYG2S+4CbGR2uehfwnCRTt80Z/76Pr4tm/inAg0ez4A7tBnZX1eeb6Q8zCpL5uF1cBNxbVQ9U1aPARxltK/Nxu5jSdjt4StvHkIJjC7CyOWPieEaDYJt6rqkzScLomex3VdVvjs3aBEyd+fA6RmMfU+0/05w9cR7w8Ngu66BV1XVVtbSqljP6e/9UVf00cCtwedNt+rqYWkeXN/3nxG/gVXU/sCvJi5umVwI7mIfbBaNDVOcleWbz72VqXcy77WJM2+1gM3Bxkuc2e3AXN21H1vfgTsuBoEuBrwB/Dvxy3/V0/F1fwWg38w7gi83rUkbHZG8Bvgr8EXBq0z+Mzjr7c+D/MjrTpPfv0cF6uRD4ePP+hcCfATuBDwEnNO0nNtM7m/kv7LvuCa+DHwC2NtvGx4DnztftAvhV4MvAncBNwAnzZbsAPshobOdRRnuiVz2V7QC4slknO4E3zOazvXJcktTKkA5VSZKOAQaHJKkVg0OS1IrBIUlqxeCQJLVicGjOS/Kt5ufyJP/0KHze2szxuzdrfvN0XM15Sb5VVScluRD4par6sRZ/dlEdvO+RJNzj0PzyduAfJvli8xyHhUnemWRL84yCfwaQ5MIkn0myidGVyCT5WJJtzbMf1k8tMKNnxHwhyZeS3NK0vT7Jbzfvlyf5VLP8W5Kc2bTf2Dwf4U+T3JPk8rFlvmWspl9t2p6V5A+az7kzyWuO1kqTpls0cxdpzriWsT2OJgAerqp/kOQE4LNJPtn0PQc4q6rubaavrKpvJHkGsCXJRxj94vVfgAuq6t7m2QbT/Ufgd6vqd5NcCbwb+MfNvDMY3SHgJYxuCfHhJBczembCuYyu9t2U5ALgdOAvq+ofNbWfMrG1IrVkcGg+uxh46dhv+6cw+k97H/BnY6EB8AtJXt28X9b0Ox349FS/qvrGIT7jfOAnmvc3Ab8+Nu9jVXUA2JFk6vbXFzev25vpk5rP+gzw75O8g9EtVz7zVL6wNAkGh+azANdU1RNu6taMhXx72vRFjB4C9EiSP2Z036On6zvTapn6+WtV9d4nFTt63OelwL9NcktVXT+BGqTWHOPQfPI3wMlj05uBn29uX0+SFzUPRZruFEaPHH0kyUsYPcoX4DbggiQrmj9/qENVf8rojr4AP81oz+FINgNXNs9hIcmSJM9Pshh4pKo+ALyT0aE0qRfucWg+uQN4LMmXGD2v+V2MHr35hea23A9wcPxh3B8CP5fkLkaP3LwNoKoeaMZJPppkAaNnH7xq2p+9htHT+t7SLP8NRyqwqj6Z5PuAz41K4lvAa4HvBd6Z5ACju6H+fLuvLk2Op+NKklrxUJUkqRWDQ5LUisEhSWrF4JAktWJwSJJaMTgkSa0YHJKkVgwOSVIr/x9HtS/J+DPIaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error de entrenamiento = 0.0\n",
      "\n",
      "Error durante la prueba = 0.5\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "import math\n",
    "\n",
    "def trainMethod(grado,eta,isShowGraph = True):\n",
    "    N = np.size(X,0)\n",
    "\n",
    "    # #Se modifica la matriz de datos original de acuerdo al grado del polinomio ingresado para el modelo\n",
    "    #grado = 3\n",
    "    X2 = potenciaPolinomio(X,grado)\n",
    "\n",
    "    #Dejamos algunas muestras para el proceso de entrenamiento y otras para evaluar qué tan bueno fue el aprendizaje del modelo\n",
    "    random.seed(1)\n",
    "    ind=np.random.permutation(N)\n",
    "    Xtrain = X2[ind[0:int(math.ceil(0.7*N))],:]\n",
    "    Xtest = X2[ind[int(math.ceil(0.7*N)):N],:]\n",
    "    Ytrain = Y[ind[0:int(math.ceil(0.7*N))]]\n",
    "    Ytest = Y[ind[int(math.ceil(0.7*N)):N]]\n",
    "\n",
    "    #Normalizamos los datos\n",
    "    media = np.mean(Xtrain)\n",
    "    desvia = np.std(Xtrain)\n",
    "    Xtrain = stats.stats.zscore(Xtrain)\n",
    "    Xtest = (Xtest - np.matlib.repmat(media, Xtest.shape[0], 1))/np.matlib.repmat(desvia, Xtest.shape[0], 1)\n",
    "\n",
    "    #eta = 1\n",
    "\n",
    "    #Complete la siguiente línea de código llamando el método gradiente_descendente con sus respectivos argumentos\n",
    "    W = gradiente_descendente_logistic(Xtrain, Ytrain, grado, eta, isShowGraph)\n",
    "\n",
    "    #Evaluamos las predicciones del modelo con los datos de test\n",
    "    unos = np.array([np.ones(np.size(Xtest,0))])\n",
    "    Xtest2 = np.concatenate((unos.T, Xtest), axis=1)\n",
    "    Xtest2 = Xtest2.reshape(np.size(Xtest2,0),np.size(Xtest2,1))\n",
    "    Yest = logistic_regression(Xtest2, W)\n",
    "    Error = error_logistic(Yest,Ytest)\n",
    "    print('\\nError durante la prueba = ' + str(Error))\n",
    "    \n",
    "trainMethod(3,1) # grado: 3  eta:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmos para automatizar la tabla\n",
    "\n",
    "para $ETA = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************** GRADO: 1 **** ETA: 1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.46\n",
      "Iteración:  200  ECM:  0.46\n",
      "Iteración:  300  ECM:  0.46\n",
      "Iteración:  400  ECM:  0.46\n",
      "Iteración:  500  ECM:  0.46\n",
      "Iteración:  600  ECM:  0.46\n",
      "Iteración:  700  ECM:  0.46\n",
      "Iteración:  800  ECM:  0.46\n",
      "Iteración:  900  ECM:  0.46\n",
      "\n",
      "Error de entrenamiento = 0.46\n",
      "\n",
      "Error durante la prueba = 0.47333333333333333\n",
      "\n",
      "*************** GRADO: 2 **** ETA: 1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.002857142857142857\n",
      "Iteración:  200  ECM:  0.0\n",
      "Iteración:  300  ECM:  0.0\n",
      "Iteración:  400  ECM:  0.0\n",
      "Iteración:  500  ECM:  0.0\n",
      "Iteración:  600  ECM:  0.0\n",
      "Iteración:  700  ECM:  0.0\n",
      "Iteración:  800  ECM:  0.0\n",
      "Iteración:  900  ECM:  0.0\n",
      "\n",
      "Error de entrenamiento = 0.0\n",
      "\n",
      "Error durante la prueba = 0.16\n",
      "\n",
      "*************** GRADO: 3 **** ETA: 1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.002857142857142857\n",
      "Iteración:  200  ECM:  0.0\n",
      "Iteración:  300  ECM:  0.0\n",
      "Iteración:  400  ECM:  0.0\n",
      "Iteración:  500  ECM:  0.0\n",
      "Iteración:  600  ECM:  0.0\n",
      "Iteración:  700  ECM:  0.0\n",
      "Iteración:  800  ECM:  0.0\n",
      "Iteración:  900  ECM:  0.0\n",
      "\n",
      "Error de entrenamiento = 0.0\n",
      "\n",
      "Error durante la prueba = 0.5\n",
      "\n",
      "*************** GRADO: 4 **** ETA: 1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.005714285714285714\n",
      "Iteración:  200  ECM:  0.002857142857142857\n",
      "Iteración:  300  ECM:  0.002857142857142857\n",
      "Iteración:  400  ECM:  0.002857142857142857\n",
      "Iteración:  500  ECM:  0.002857142857142857\n",
      "Iteración:  600  ECM:  0.002857142857142857\n",
      "Iteración:  700  ECM:  0.002857142857142857\n",
      "Iteración:  800  ECM:  0.002857142857142857\n",
      "Iteración:  900  ECM:  0.0\n",
      "\n",
      "Error de entrenamiento = 0.0\n",
      "\n",
      "Error durante la prueba = 0.12666666666666668\n",
      "\n",
      "*************** GRADO: 5 **** ETA: 1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.005714285714285714\n",
      "Iteración:  200  ECM:  0.002857142857142857\n",
      "Iteración:  300  ECM:  0.002857142857142857\n",
      "Iteración:  400  ECM:  0.002857142857142857\n",
      "Iteración:  500  ECM:  0.002857142857142857\n",
      "Iteración:  600  ECM:  0.002857142857142857\n",
      "Iteración:  700  ECM:  0.002857142857142857\n",
      "Iteración:  800  ECM:  0.002857142857142857\n",
      "Iteración:  900  ECM:  0.0\n",
      "\n",
      "Error de entrenamiento = 0.0\n",
      "\n",
      "Error durante la prueba = 0.5\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"\\n*************** GRADO: \"+str(i)+\" **** ETA: \"+str(eta)+\" ******************\\n\")\n",
    "    trainMethod(i,eta,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para $ETA=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************** GRADO: 1 **** ETA: 0.1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.46\n",
      "Iteración:  200  ECM:  0.46\n",
      "Iteración:  300  ECM:  0.46\n",
      "Iteración:  400  ECM:  0.46\n",
      "Iteración:  500  ECM:  0.46\n",
      "Iteración:  600  ECM:  0.46\n",
      "Iteración:  700  ECM:  0.46\n",
      "Iteración:  800  ECM:  0.46\n",
      "Iteración:  900  ECM:  0.46\n",
      "\n",
      "Error de entrenamiento = 0.46\n",
      "\n",
      "Error durante la prueba = 0.47333333333333333\n",
      "\n",
      "*************** GRADO: 2 **** ETA: 0.1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.014285714285714285\n",
      "Iteración:  200  ECM:  0.008571428571428572\n",
      "Iteración:  300  ECM:  0.008571428571428572\n",
      "Iteración:  400  ECM:  0.008571428571428572\n",
      "Iteración:  500  ECM:  0.005714285714285714\n",
      "Iteración:  600  ECM:  0.005714285714285714\n",
      "Iteración:  700  ECM:  0.005714285714285714\n",
      "Iteración:  800  ECM:  0.005714285714285714\n",
      "Iteración:  900  ECM:  0.002857142857142857\n",
      "\n",
      "Error de entrenamiento = 0.002857142857142857\n",
      "\n",
      "Error durante la prueba = 0.12666666666666668\n",
      "\n",
      "*************** GRADO: 3 **** ETA: 0.1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.014285714285714285\n",
      "Iteración:  200  ECM:  0.008571428571428572\n",
      "Iteración:  300  ECM:  0.008571428571428572\n",
      "Iteración:  400  ECM:  0.005714285714285714\n",
      "Iteración:  500  ECM:  0.005714285714285714\n",
      "Iteración:  600  ECM:  0.005714285714285714\n",
      "Iteración:  700  ECM:  0.005714285714285714\n",
      "Iteración:  800  ECM:  0.002857142857142857\n",
      "Iteración:  900  ECM:  0.002857142857142857\n",
      "\n",
      "Error de entrenamiento = 0.002857142857142857\n",
      "\n",
      "Error durante la prueba = 0.5\n",
      "\n",
      "*************** GRADO: 4 **** ETA: 0.1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.022857142857142857\n",
      "Iteración:  200  ECM:  0.014285714285714285\n",
      "Iteración:  300  ECM:  0.014285714285714285\n",
      "Iteración:  400  ECM:  0.014285714285714285\n",
      "Iteración:  500  ECM:  0.014285714285714285\n",
      "Iteración:  600  ECM:  0.014285714285714285\n",
      "Iteración:  700  ECM:  0.011428571428571429\n",
      "Iteración:  800  ECM:  0.008571428571428572\n",
      "Iteración:  900  ECM:  0.008571428571428572\n",
      "\n",
      "Error de entrenamiento = 0.005714285714285714\n",
      "\n",
      "Error durante la prueba = 0.03333333333333333\n",
      "\n",
      "*************** GRADO: 5 **** ETA: 0.1 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.022857142857142857\n",
      "Iteración:  200  ECM:  0.014285714285714285\n",
      "Iteración:  300  ECM:  0.014285714285714285\n",
      "Iteración:  400  ECM:  0.014285714285714285\n",
      "Iteración:  500  ECM:  0.014285714285714285\n",
      "Iteración:  600  ECM:  0.014285714285714285\n",
      "Iteración:  700  ECM:  0.011428571428571429\n",
      "Iteración:  800  ECM:  0.008571428571428572\n",
      "Iteración:  900  ECM:  0.008571428571428572\n",
      "\n",
      "Error de entrenamiento = 0.005714285714285714\n",
      "\n",
      "Error durante la prueba = 0.5\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"\\n*************** GRADO: \"+str(i)+\" **** ETA: \"+str(eta)+\" ******************\\n\")\n",
    "    trainMethod(i,eta,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para $ETA=0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"\\n*************** GRADO: \"+str(i)+\" **** ETA: \"+str(eta)+\" ******************\\n\")\n",
    "    trainMethod(i,eta,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************** GRADO: 1 **** ETA: 0.001 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.46\n",
      "Iteración:  200  ECM:  0.46\n",
      "Iteración:  300  ECM:  0.46\n",
      "Iteración:  400  ECM:  0.46\n",
      "Iteración:  500  ECM:  0.46\n",
      "Iteración:  600  ECM:  0.46\n",
      "Iteración:  700  ECM:  0.46\n",
      "Iteración:  800  ECM:  0.46\n",
      "Iteración:  900  ECM:  0.46\n",
      "\n",
      "Error de entrenamiento = 0.46\n",
      "\n",
      "Error durante la prueba = 0.48\n",
      "\n",
      "*************** GRADO: 2 **** ETA: 0.001 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.014285714285714285\n",
      "Iteración:  200  ECM:  0.014285714285714285\n",
      "Iteración:  300  ECM:  0.014285714285714285\n",
      "Iteración:  400  ECM:  0.014285714285714285\n",
      "Iteración:  500  ECM:  0.014285714285714285\n",
      "Iteración:  600  ECM:  0.014285714285714285\n",
      "Iteración:  700  ECM:  0.014285714285714285\n",
      "Iteración:  800  ECM:  0.014285714285714285\n",
      "Iteración:  900  ECM:  0.014285714285714285\n",
      "\n",
      "Error de entrenamiento = 0.014285714285714285\n",
      "\n",
      "Error durante la prueba = 0.08\n",
      "\n",
      "*************** GRADO: 3 **** ETA: 0.001 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.014285714285714285\n",
      "Iteración:  200  ECM:  0.014285714285714285\n",
      "Iteración:  300  ECM:  0.014285714285714285\n",
      "Iteración:  400  ECM:  0.014285714285714285\n",
      "Iteración:  500  ECM:  0.014285714285714285\n",
      "Iteración:  600  ECM:  0.014285714285714285\n",
      "Iteración:  700  ECM:  0.014285714285714285\n",
      "Iteración:  800  ECM:  0.014285714285714285\n",
      "Iteración:  900  ECM:  0.014285714285714285\n",
      "\n",
      "Error de entrenamiento = 0.014285714285714285\n",
      "\n",
      "Error durante la prueba = 0.13333333333333333\n",
      "\n",
      "*************** GRADO: 4 **** ETA: 0.001 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.03428571428571429\n",
      "Iteración:  200  ECM:  0.03428571428571429\n",
      "Iteración:  300  ECM:  0.03428571428571429\n",
      "Iteración:  400  ECM:  0.03428571428571429\n",
      "Iteración:  500  ECM:  0.03428571428571429\n",
      "Iteración:  600  ECM:  0.03428571428571429\n",
      "Iteración:  700  ECM:  0.03428571428571429\n",
      "Iteración:  800  ECM:  0.03428571428571429\n",
      "Iteración:  900  ECM:  0.03428571428571429\n",
      "\n",
      "Error de entrenamiento = 0.03428571428571429\n",
      "\n",
      "Error durante la prueba = 0.06\n",
      "\n",
      "*************** GRADO: 5 **** ETA: 0.001 ******************\n",
      "\n",
      "Iteración:  0  ECM:  1.0\n",
      "Iteración:  100  ECM:  0.037142857142857144\n",
      "Iteración:  200  ECM:  0.037142857142857144\n",
      "Iteración:  300  ECM:  0.037142857142857144\n",
      "Iteración:  400  ECM:  0.037142857142857144\n",
      "Iteración:  500  ECM:  0.03428571428571429\n",
      "Iteración:  600  ECM:  0.03428571428571429\n",
      "Iteración:  700  ECM:  0.03428571428571429\n",
      "Iteración:  800  ECM:  0.03428571428571429\n",
      "Iteración:  900  ECM:  0.03428571428571429\n",
      "\n",
      "Error de entrenamiento = 0.03428571428571429\n",
      "\n",
      "Error durante la prueba = 0.04\n"
     ]
    }
   ],
   "source": [
    "eta = 0.001\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"\\n*************** GRADO: \"+str(i)+\" **** ETA: \"+str(eta)+\" ******************\\n\")\n",
    "    trainMethod(i,eta,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabla de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac74ed5a8084ef1a0ff667d124e7092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defau…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import qgrid\n",
    "randn = np.random.randn\n",
    "df_types = pd.DataFrame({\n",
    "    'Tasa de aprendizaje' : pd.Series(['1', '1', '1', '1', '1', '0.1', '0.1', '0.1', '0.1', '0.1', '0.001', '0.001', '0.001', '0.001', '0.001']),\n",
    "    'Grado del polinomio' : pd.Series([1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])})\n",
    "df_types[\"Error_Entrenamiento\"] = \"\"\n",
    "df_types[\"Error_Prueba\"] = \"\"\n",
    "df_types.set_index(['Tasa de aprendizaje','Grado del polinomio'], inplace=True)\n",
    "df_types[\"Error_Entrenamiento\"][2] = \"0.0\"\n",
    "df_types[\"Error_Prueba\"][2] = \"0.5\"\n",
    "#df_types.sort_index(inplace=True)\n",
    "qgrid_widget = qgrid.show_grid(df_types, show_toolbar=False)\n",
    "qgrid_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecute la siguiente instrucción para dejar guardados en el notebook los resultados de las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Error_Entrenamiento</th>\n",
       "      <th>Error_Prueba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tasa de aprendizaje</th>\n",
       "      <th>Grado del polinomio</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.001</th>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Error_Entrenamiento Error_Prueba\n",
       "Tasa de aprendizaje Grado del polinomio                                 \n",
       "1                   1                                  0.46        0.473\n",
       "                    2                                   0.0         0.16\n",
       "                    3                                   0.0          0.5\n",
       "                    4                                   0.0        0.126\n",
       "                    5                                   0.0          0.5\n",
       "0.1                 1                                  0.46        0.473\n",
       "                    2                                 0.003        0.127\n",
       "                    3                                 0.003          0.5\n",
       "                    4                                 0.006        0.033\n",
       "                    5                                 0.006          0.5\n",
       "0.001               1                                  0.46         0.48\n",
       "                    2                                 0.014         0.08\n",
       "                    3                                 0.014        0.133\n",
       "                    4                                 0.014        0.133\n",
       "                    5                                 0.034         0.06"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qgrid_widget.get_changed_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "4.1 Escriba el modelo $f(\\textbf{x},\\textbf{w})$, de la mejor frontera de decisión que encontró según la tabla de resultados.\n",
    "\n",
    "4.2 Basado en el valor del error obtenido, ¿cu&aacute;ntas muestras de entrenamiento y de prueba clasifica mal el modelo? (un valor para cada conjunto). Nota. Escriba en una celda el código con el cuál obtuvo la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Modelo $f(\\textbf{x},\\textbf{w})$ = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código utiliza un clasificador basado en Funciones Discriminantes Gaussianas para resolver el mismo problema de clasificación. Ejecute el código y responda las siguientes preguntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistribucionGaussiana(X,Mu,Sigma):\n",
    "    \n",
    "    SigmaInversa = np.linalg.inv(np.array(Sigma))\n",
    "    PrimerTermino = (1/(2*math.pi*math.sqrt(np.linalg.det(Sigma))))\n",
    "    \n",
    "    primerDot = np.dot((X-Mu),SigmaInversa)\n",
    "    segundoDot = np.dot(primerDot,(X-Mu).T)\n",
    "    Exponencial = math.exp(-0.5*segundoDot)\n",
    "    \n",
    "    Probabilidad = PrimerTermino * Exponencial\n",
    "    \n",
    "    return Probabilidad\n",
    "\n",
    "def FuncionDiscriminanteG(Xtrain,Ytrain,Xtest,tipo):\n",
    "    \n",
    "    N = Xtest.shape[0]\n",
    "    #Estimación de medias y Covarianzas\n",
    "    Mu1 = np.mean(Xtrain[(Ytrain==1).flat,:], axis=0)\n",
    "    Mu2 = np.mean(Xtrain[(Ytrain==0).flat,:], axis=0)\n",
    "  \n",
    "    Sigma1 = np.cov((Xtrain[(Ytrain==1).flat,:]).T)\n",
    "    Sigma2 = np.cov((Xtrain[(Ytrain==0).flat,:]).T)\n",
    "    \n",
    "    Sigma3 = (0.5*(Sigma1+Sigma2))\n",
    "    Yest = np.zeros(N)\n",
    "    Tipo = tipo\n",
    "    for i in range(N):\n",
    "        \n",
    "            if Tipo == 0 :\n",
    "                p1 = DistribucionGaussiana(Xtest[i,:],Mu1,Sigma1)\n",
    "                p2 = DistribucionGaussiana(Xtest[i,:],Mu2,Sigma2)\n",
    "            elif Tipo == 1:\n",
    "                p1 = DistribucionGaussiana(Xtest[i,:],Mu1,Sigma3)\n",
    "                p2 = DistribucionGaussiana(Xtest[i,:],Mu2,Sigma3)\n",
    "            if p1 >= p2:\n",
    "                Yest[i] = 1\n",
    "            else:\n",
    "                Yest[i] = 0\n",
    "                \n",
    "    return Yest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La eficiencia en esta iteración fue: 0.6599999999999999\n",
      "\n",
      "\n",
      "Error prueba (Frontera Lineal) = 0.34\n",
      "La eficiencia en esta iteración fue: 0.8666666666666667\n",
      "\n",
      "\n",
      "Error prueba (Frontera cuadrática) = 0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "tipo = 0 # Frontera lineal\n",
    "Yest0 = FuncionDiscriminanteG(Xtrain,Ytrain,Xtest,tipo)\n",
    "Error = error_logistic(Yest0,Ytest)\n",
    "print('\\nError prueba (Frontera Lineal) = ' + str(Error))\n",
    "\n",
    "\n",
    "tipo = 1 #Frontera cuadrática\n",
    "Yest1 = FuncionDiscriminanteG(Xtrain,Ytrain,Xtest,tipo)\n",
    "Error = error_logistic(Yest1,Ytest)\n",
    "print('\\nError prueba (Frontera cuadrática) = ' + str(Error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 ¿Cuál tipo de frontera proporcionó mejores resultados?:\n",
    "\n",
    "5.2 Teniendo en cuenta la forma de los datos (De acuerdo con la gráfica hecha en el punto 2), expliqué porqué el modelo de Funciones Discriminantes Gaussianas obtiene un buen resultado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
