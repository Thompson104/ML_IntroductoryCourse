{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente decendente\n",
    "$f(x) = W_0+W_1X_1+W_2X_2+...+W_jX_j$\n",
    "\n",
    "\n",
    "$$E(W)=\\frac {1}{2N} \\sum_{i=1}^N 2(f(X_i)-Y_i)^2$$\n",
    "\n",
    "\n",
    "$$(\\partial E)/(\\partial W_j) =\\frac {1}{2N}\\sum_{i=1}^N 2(f(X_i)-Y_i)^2$$\n",
    "\n",
    "algoritmo general para GD\n",
    "\n",
    "    W = zero(d) //vector de ceros\n",
    "    eta = 1\n",
    "    MaxIter = 100\n",
    "\n",
    "    for iter in range(MaxIter):\n",
    "        for j in range(d):\n",
    "            Sum += W*X[1,.] //X[1,.]: sum/N\n",
    "            W[j] = W[j] - eta\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "Perror = 0.3\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "x1 = np.linspace(-10,10,100).reshape(100, 1)\n",
    "x2 = x1**2\n",
    "y = 3*x1 + 7*x2 - 2\n",
    "y2 = y + Perror*np.std(y)*(np.random.rand(100,1) - 0.5)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Y = 3*X1 + 7*X2 - 2\n",
    "Y2 = y + Perror*np.std(y)*(np.random.rand(100,100) - 0.5)\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "p=ax.plot_surface(X1, X2, Y, rstride=2, cstride=2, cmap=cm.jet, alpha=0.7)\n",
    "\n",
    "## surface_plot with color grading and color bar\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "p = ax.plot_surface(X1, X2, Y2, rstride=2, cstride=2, cmap=cm.jet, alpha=0.7)\n",
    "cb = fig.colorbar(p, shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## haciendo el gradiente decendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxIter = 100000\n",
    "w = np.ones(3).reshape(3, 1)\n",
    "eta = 0.0001\n",
    "N = len(x1)\n",
    "Error =np.zeros(MaxIter)\n",
    "X = np.array([x1,x2,np.ones((100,1))]).reshape(3, 100);\n",
    "for i in range(MaxIter):\n",
    "    tem = np.dot(X.T,w)\n",
    "    tem2 = tem-np.array(y2)\n",
    "    Error[i] = np.sum(tem2**2)/(2*N)\n",
    "    tem = np.dot(X,tem2)\n",
    "    wsig = w - eta*tem/N\n",
    "    w = wsig\n",
    "print(w)\n",
    "print(' Error = ',Error[-1])\n",
    "plt.ylim(0,1000)\n",
    "plt.xlim(0,200)\n",
    "plt.ion()\n",
    "plt.plot(np.linspace(0,MaxIter,MaxIter),Error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion logistica\n",
    "\n",
    "$f(x)= -x_2 + x_1 - 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from sklearn import datasets\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "def sigmoide(u):\n",
    "    return np.exp(u)/(1 + np.exp(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def GradientSigmo(MaxIter=10000, eta = 0.001):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.axis([None, None, 0, 100])\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X2 = X[:100][:,:2]\n",
    "    y2 = y[:100]\n",
    "    #Aprendizaje\n",
    "    w = np.ones(3).reshape(3, 1)\n",
    "    N = len(y2)\n",
    "    Error =np.zeros(MaxIter)\n",
    "    Xent = np.concatenate((X2,np.ones((100,1))),axis=1)\n",
    "\n",
    "    for i in range(MaxIter):\n",
    "        tem = np.dot(Xent,w)\n",
    "        tem2 = sigmoide(tem.T)-np.array(y2)\n",
    "        Error[i] = np.sum(np.abs(tem2))/N\n",
    "        tem = np.dot(Xent.T,tem2.T)\n",
    "        wsig = w - eta*tem/N\n",
    "        w = wsig\n",
    "        \n",
    "        #plt.subplot(1,2,2)\n",
    "        iris = datasets.load_iris()\n",
    "        X, y = iris.data, iris.target\n",
    "        X2 = X[:100][:,:2]\n",
    "        y2 = y[:100]\n",
    "        plt.scatter(X2[:,0], X2[:,1], c=y2,cmap=\"Accent\");\n",
    "        x1 = np.linspace(4,8,20)\n",
    "        x2 = -(w[0]/w[1])*x1 - (w[2]/w[1])\n",
    "\n",
    "        plt.plot(x1,x2)\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "\n",
    "\n",
    "    print(w)\n",
    "    print('Error=',Error[-1])\n",
    "    #Grafica de la frontera encontrada\n",
    "    plt.subplot(1,2,2)\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X2 = X[:100][:,:2]\n",
    "    y2 = y[:100]\n",
    "    plt.scatter(X2[:,0], X2[:,1], c=y2,cmap=\"Accent\");\n",
    "    x1 = np.linspace(4,8,20)\n",
    "    x2 = -(w[0]/w[1])*x1 - (w[2]/w[1])\n",
    "    \n",
    "    plt.plot(x1,x2)\n",
    "    #plt.show()\n",
    "    print(MaxIter,eta)\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "interact(GradientSigmo,MaxIter=[1,10,100,1000,10000], eta=[0.01,0.1,1,10]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clase 9 de Agosto\n",
    "\n",
    "![title](\"img/kernel-trick.jpeg\")\n",
    "\n",
    "funcion de probabilidad normal\n",
    "\n",
    "$$p(x)=\\frac{1}{\\sqrt{2\\pi }} exp(-\\frac{1}{2}\\frac{(x-\\mu )^2)}{^{\\sigma ^2}})$$\n",
    "\n",
    "pero como necesitamos que la funcion sea de varias variables queda dada por\n",
    "$$p(x)=\\frac{1}{(2\\pi)^\\frac{d}{2} \\left | \\sum  \\right | ^\\frac{1}{2}} exp(-\\frac{1}{2} (x-\\mu )^T \\sum ^{-1} (x-\\mu ))$$\n",
    "\n",
    "donde $\\left | \\sum  \\right |$ es la Matriz de covarianza $[d\\times d]$ y $\\mu$ es el vector de medias $[d \\times]$\n",
    "\n",
    "verosimilitud lo que hace es tratar de encontrar una funcion de probabilidad que se ajuste a los datos que se tienen, osea mas cercana a la realidad\n",
    "\n",
    "## Maximo Verosimilitud\n",
    "\n",
    "$$max  p(x_{1},x_{2},x_{3},...,x_{n})$$\n",
    "\n",
    "si las muestras son independientes\n",
    "\n",
    "$$max \\prod_{i=1}^{N} p(x_{i})$$\n",
    "\n",
    "$$\\iota  = \\sum_{i=1}^{N} log(p(x_{i}))$$\n",
    "\n",
    "....\n",
    "\n",
    "contunua reemplazando  xi por la funcion de probabilidad y haceindo la derivada con rescpecto a mu obtenemos la ecuacion de la media\n",
    "\n",
    "\n",
    "\n",
    "## Discriminativo\n",
    "se refiere a una frontera\n",
    "\n",
    "## Generativo\n",
    "fdp x clase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Modelos no parametricos\n",
    "\n",
    "suponemos que los datos se comportan como una gausiana, si se hace la suposicion de que los datos se comportan como una gausiana seria un pa\n",
    "\n",
    "\n",
    "para normalizar $z-score$\n",
    "\n",
    "### No parametrico:\n",
    "Modelo con un número de parametros que incrementa con la cantidad de muestras\n",
    "\n",
    "### Paramétrico\n",
    "Modelo con un número constante de parametros.\n",
    "\n",
    "## Modelo Kernel\n",
    "$$f({\\bf{x}}^*) = \\frac{1}{N h^d} \\sum_{i=1}^{N} K\\left(\\frac{{\\bf{x}}^* - {\\bf{x}}_i}{h}\\right)$$\n",
    "\n",
    "$ k(u) = exp(\\frac{u^2}{2})$ ----> similitud\n",
    "la anterior funcion nos da una campana de gaus con maximo 1 $exp(0)=1$\n",
    "\n",
    "$\\bf{x}_i$ corresponde para el problema de clasificacion a la clase que se esta evaluando\n",
    "\n",
    "si la ventana es angosta le da peso a los que están muy cerca, pero si la ventana es mas grande le da el mismo peso a distancias mas alejadas\n",
    "\n",
    "$\\bf{x}^* - \\bf{x}_i$ siendo la distancia $||\\bf{x}^* - \\bf{x} ||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 21 de Agosto de 2018\n",
    "\n",
    "### MAPE: Mean Absolute Percentage Error\n",
    "$$E=\\frac{1}{N} \\sum_{i=1}^{N}\\left | \\frac{f(x_i)-y_i)}{y_i} \\right |$$\n",
    "es el porcentaje de error con respecto a la salida, si el error es 0.01 % el promedio de error en regresion es de el puntouno porciento de la salida.\n",
    "\n",
    "### MEA: Mean Absolute Error\n",
    "\n",
    "$$E=\\frac{1}{N} \\sum_{i=1}^{N}\\left | f(x_i)-y_i) \\right |$$\n",
    "\n",
    "### Error cuadratico medio\n",
    "$$E=\\frac{1}{N} \\sum_{i=1}^{N} (f(x_i)-y_i)^2 $$\n",
    "\n",
    "### Coeficiente de determinacion\n",
    "\n",
    "\n",
    "Estos tres errores son las mas usuadas para evaluar el modelo\n",
    "\n",
    "\n",
    "los corchetes raros es una funcion indicador: toma unos cuando el sistema fallo\n",
    "$$E=\\frac{1}{N} \\sum_{i=1}^{N} \\left \\| f(x_i)-y_i \\right \\|$$\n",
    "\n",
    "#### True Positive:\n",
    "son las muestras positivas que el sistema clasificó correctamente\n",
    "\n",
    "#### True Negatice:\n",
    "son las muestras negativas que el sistema clasificó correctamente\n",
    "\n",
    "#### False Positive:\n",
    "son las muestras negativas que el sistema clasificó como positivas\n",
    "\n",
    "#### False Negative:\n",
    "son las muetras positivas que el sistema clasificó como negativas\n",
    "\n",
    "##### EJEMPLO: Sistema Seguridad\n",
    "POSITIVO -> personas que tiene derecho de acceder al sistema\n",
    "\n",
    "critico un falso positivo\n",
    "\n",
    "##### EJEMPLO: Sistema de Oncologia\n",
    "\n",
    "POSITIVO -> persona con cancer\n",
    "\n",
    "critico es un falso negativo\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>1</th><th>2</th><th>3</th><th>...</th><th>C</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td><td></td><td></td><td></td><td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>...</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Sensibilidad $\\frac{TP}{TP + FN}$ Recall\n",
    "\n",
    "\n",
    "Especificidad $\\frac{TN}{TN + FP}$\n",
    "\n",
    "\n",
    "Eficiencia: $\\frac{TP+TN}{TP + TN + FP + FN}$\n",
    "\n",
    "\n",
    "Precision: $\\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "Error = 1 - Eficiencia\n",
    "\n",
    "\n",
    "En la matriz de confusion, si cojemos la clase 1 en la posicion 1,1 si lo dividimos por la suma de las demas terminos de la columna 1 eso nos da el error de la clase 1\n",
    "\n",
    "<img src=\"./img/confuxion_matrix.png\" whidth=\"50%\">\n",
    "\n",
    "<b>Figura:</b> imagen tomanda de http://scikit-learn.org/\n",
    "\n",
    "### Métodologias de Validación\n",
    "\n",
    "- Evaluar el desempeño real del sistema\n",
    "- seleccionar los hipeparametros del modelo\n",
    "\n",
    "#### Validación cruzada:\n",
    "\n",
    "##### Crossvalidation\n",
    "dividimos el conjuntos en k subconjuntos disyuntos, sacamos una cantidad de datos y entrenamos sin ese valor por las k veces. medimos todo y calculamos los promedios\n",
    "\n",
    "- Promedio de cada una de las medidas de error\n",
    "- Desviacion estandar de cada una de las medidas de error\n",
    "\n",
    "\n",
    "<table><tr> <th>0.01</th> <th>$Sensibilidad \\pm \\sigma$</th> <th>$Especificidad \\pm \\sigma$</th> <th>$Acuorry \\pm \\sigma$</th> </tr>\n",
    "<tr><td>0.1</td> <td></td> <td></td> <td></td> <td></td></tr>\n",
    "    <tr><td>1</td>  <td></td> <td></td> <td></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "#### Booststrapping\n",
    "\n",
    "dividimos por porcentajes lo mas usual es $70 \\percent$ por 10 veces\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>0.01</th> <th>$Sensibilidad \\pm \\sigma$</th> \n",
    "        <th>$Especificidad \\pm \\sigma$</th> <th>$Acuorry \\pm \\sigma$</th> </tr>\n",
    "    <tr>\n",
    "        <td>0.1</td> <td></td> <td></td> <td></td> <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>  <td></td> <td></td> <td></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Desbalanceo \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 23 de Agosto\n",
    "\n",
    "## Imbalanced learning\n",
    "\n",
    "- Controlar el particionamiento de las muestras: utilizar una estrategia de validacion estratificada.\n",
    "\n",
    "## Multi-instance leaning\n",
    "en este caso cada muestra a predecir está compuesta por varios vectores de caracteristicas (instancias) en lugar de solo una.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x & x &x \\\\ \n",
    "x & x &x \\\\ \n",
    "x & x &x \\\\ \n",
    "x & x &x \\\\ \n",
    "x & x &x\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "donde las filas con muestras y las columnas son caracteriesticas.\n",
    "- Para transformar nuestro problema para hacerlo mas sencillo se puede hacer un promedio de todos los vectores.\n",
    "- Una matriz de cada region, no se puede particionar para el entrenamiento, porque no son independientes por que pertenecen a la misma region.\n",
    "se necesitan tres vectores, uno que me diga a que region pertenece, otro con las salidas y las muestras.\n",
    "\n",
    "<img src=\"./img/conjunto.png\">\n",
    "\n",
    "### Submuestreo:\n",
    "Es quitar muestras de forma correcta, muestras redundantes, datos atipicos.\n",
    "\n",
    "### Sobremuestreo (Oversampling)\n",
    "Es agregar muestras a la clase minoritaria\n",
    "\n",
    "#### Tecnica para generar muestras\n",
    "https://jair.org/index.php/jair/article/view/10302/24590\n",
    "\n",
    "### Aprendisaje con balance de costo\n",
    "$p(x|c) = p(c|x)$ esto es erroneo por bayes\n",
    "$p(c|x) = \\frac{p(x|c) p(c)}{p(x)}$\n",
    "\n",
    "$p(x|c_1) > p(x|c_2)$ esto seria $p(x|c_1) p(c_1) > p(x|c_2) p(c2)$ queremos encontrar $?$ en \n",
    "$\\frac{p(x|c_1)}{p(x|c_2)} = ?    (1)$ \n",
    "\n",
    "el score $SCORE = log \\frac{p(x|c_1)}{p(x|c_2)}$\n",
    "\n",
    "<img src=\"./img/tablero2308.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
